"""
CodePulse AI Engine
======================

AI-powered code analysis using Claude API. This was the most exciting part to build!
It sends code to the AI and gets back intelligent suggestions.

The AI can spot issues I might miss and suggest improvements. It's like having
a senior developer review your code (well, almost!).

Author: Saleh Almqati
License: MIT
"""

import os
import json
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum

logger = logging.getLogger(__name__)


class Severity(Enum):
    """Issue severity levels"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class IssueCategory(Enum):
    """Categories of code issues"""
    SECURITY = "security"
    PERFORMANCE = "performance"
    LOGIC = "logic"
    STYLE = "style"
    TESTING = "testing"
    DOCUMENTATION = "documentation"


@dataclass
class CodeIssue:
    """Represents an issue found in the code"""
    file_path: str
    line_number: int
    severity: Severity
    category: IssueCategory
    title: str
    description: str
    suggestion: str
    code_snippet: Optional[str] = None
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        data = asdict(self)
        data['severity'] = self.severity.value
        data['category'] = self.category.value
        return data


@dataclass
class AnalysisResult:
    """Result of AI code analysis"""
    file_path: str
    overall_score: float  # 0-100
    issues: List[CodeIssue]
    suggestions: List[str]
    strengths: List[str]
    complexity_assessment: str
    
    def to_dict(self) -> Dict:
        """Convert to dictionary"""
        return {
            'file_path': self.file_path,
            'overall_score': self.overall_score,
            'issues': [issue.to_dict() for issue in self.issues],
            'suggestions': self.suggestions,
            'strengths': self.strengths,
            'complexity_assessment': self.complexity_assessment
        }


class AIEngine:
    """
    AI-powered code analysis engine using LLMs.
    
    This engine provides intelligent code review by:
    - Understanding code context and intent
    - Identifying subtle bugs and edge cases
    - Suggesting performance optimizations
    - Recommending best practices
    - Predicting potential issues before they occur
    """
    
    def __init__(self, api_key: Optional[str] = None, model: str = "claude-sonnet-4"):
        """
        Initialize the AI engine.
        
        Args:
            api_key: API key for the LLM provider (or from environment)
            model: Model identifier to use
        """
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')
        self.model = model
        
        if not self.api_key:
            logger.warning("No API key provided. AI features will be limited.")
        else:
            logger.info(f"Initialized AI engine with model: {model}")
    
    def _build_analysis_prompt(self, code: str, file_path: str, context: Dict[str, Any]) -> str:
        """
        Build a comprehensive prompt for code analysis.
        
        Args:
            code: Source code to analyze
            file_path: Path to the file
            context: Additional context (imports, dependencies, etc.)
            
        Returns:
            Formatted prompt string
        """
        prompt = f"""You are an expert code reviewer and software architect performing a comprehensive analysis of the following code.

**File:** `{file_path}`

**Code:**
```python
{code}
```

**Context:**
- Imports: {', '.join(context.get('imports', []))}
- Functions: {len(context.get('functions', []))}
- Classes: {len(context.get('classes', []))}
- Complexity Score: {context.get('complexity', 0):.1f}/100

Please analyze this code thoroughly and provide:

1. **Critical Issues**: Security vulnerabilities, logic errors, or bugs that could cause failures
2. **Performance Issues**: Inefficiencies, bottlenecks, or optimization opportunities
3. **Code Quality**: Style issues, maintainability concerns, or best practice violations
4. **Testing Gaps**: Missing tests, edge cases not covered, or test improvements needed
5. **Overall Assessment**: Strengths of the code and a quality score (0-100)

For each issue, provide:
- Severity (critical/high/medium/low)
- Category (security/performance/logic/style/testing)
- Line number (if applicable)
- Clear description of the problem
- Specific suggestion for improvement
- Example code if helpful

Focus on actionable, specific feedback that helps improve the code. Be constructive and explain the "why" behind your recommendations.

Format your response as JSON with this structure:
{{
    "overall_score": 85,
    "issues": [
        {{
            "line_number": 42,
            "severity": "high",
            "category": "security",
            "title": "Brief title",
            "description": "Detailed description",
            "suggestion": "How to fix it"
        }}
    ],
    "suggestions": ["General improvement suggestions"],
    "strengths": ["What the code does well"],
    "complexity_assessment": "Assessment of code complexity"
}}
"""
        return prompt
    
    def _call_llm(self, prompt: str) -> str:
        """
        Make API call to the LLM.
        
        Args:
            prompt: The prompt to send
            
        Returns:
            LLM response as string
        """
        # This is a placeholder for actual API integration
        # In production, this would call Anthropic's Claude API
        
        if not self.api_key:
            logger.warning("API key not available. Returning mock response.")
            return self._get_mock_response()
        
        try:
            # TODO: Implement actual API call
            # Example structure:
            # import anthropic
            # client = anthropic.Anthropic(api_key=self.api_key)
            # response = client.messages.create(
            #     model=self.model,
            #     max_tokens=4096,
            #     messages=[{"role": "user", "content": prompt}]
            # )
            # return response.content[0].text
            
            logger.info("Making API call to LLM...")
            return self._get_mock_response()
            
        except Exception as e:
            logger.error(f"Error calling LLM: {e}")
            return self._get_mock_response()
    
    def _get_mock_response(self) -> str:
        """
        Generate a mock response for testing without API key.
        
        Returns:
            Mock JSON response
        """
        mock_response = {
            "overall_score": 78,
            "issues": [
                {
                    "line_number": 25,
                    "severity": "high",
                    "category": "performance",
                    "title": "Inefficient loop structure",
                    "description": "The nested loop has O(nÂ²) complexity which will become slow with large datasets. When processing more than 1000 items, this could cause significant delays.",
                    "suggestion": "Consider using a dictionary or set for O(1) lookups instead of nested iteration. Example: create a lookup dict before the loop and use 'if item in lookup_dict' instead of the inner loop."
                },
                {
                    "line_number": 42,
                    "severity": "medium",
                    "category": "security",
                    "title": "Potential SQL injection vulnerability",
                    "description": "Direct string concatenation for SQL query construction can lead to SQL injection attacks if user input is not properly sanitized.",
                    "suggestion": "Use parameterized queries or an ORM like SQLAlchemy. Replace: query = f\"SELECT * FROM users WHERE id={user_id}\" with: query = \"SELECT * FROM users WHERE id=?\" and pass user_id as parameter."
                },
                {
                    "line_number": 67,
                    "severity": "low",
                    "category": "style",
                    "title": "Missing type hints",
                    "description": "Function parameters lack type annotations, making the code less maintainable and harder for IDEs to provide assistance.",
                    "suggestion": "Add type hints: def process_data(items: List[Dict[str, Any]], threshold: float) -> Dict[str, int]:"
                }
            ],
            "suggestions": [
                "Consider adding comprehensive docstrings to all public functions",
                "Implement input validation at the function entry points",
                "Add unit tests for edge cases (empty lists, None values, extreme numbers)",
                "Extract magic numbers into named constants at module level",
                "Consider using dataclasses for structured data instead of plain dictionaries"
            ],
            "strengths": [
                "Good separation of concerns with clear function responsibilities",
                "Consistent naming conventions throughout the code",
                "Error handling is present in critical sections",
                "Code is generally readable with good variable names"
            ],
            "complexity_assessment": "Moderate complexity. The code handles multiple concerns but maintains reasonable structure. The main complexity driver is the data processing logic which could benefit from breaking into smaller helper functions. Overall maintainability is good with room for improvement in testability."
        }
        return json.dumps(mock_response, indent=2)
    
    def analyze_code(self, code: str, file_path: str, context: Dict[str, Any]) -> AnalysisResult:
        """
        Perform AI-powered analysis of code.
        
        Args:
            code: Source code to analyze
            file_path: Path to the file
            context: Additional context about the code
            
        Returns:
            AnalysisResult object with findings and recommendations
        """
        logger.info(f"Analyzing {file_path} with AI engine...")
        
        # Build the analysis prompt
        prompt = self._build_analysis_prompt(code, file_path, context)
        
        # Get AI response
        response_text = self._call_llm(prompt)
        
        # Parse the response
        try:
            response_data = json.loads(response_text)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response: {e}")
            # Return a default result
            return AnalysisResult(
                file_path=file_path,
                overall_score=50.0,
                issues=[],
                suggestions=["AI analysis failed. Using fallback analysis."],
                strengths=[],
                complexity_assessment="Unable to assess"
            )
        
        # Convert issues to CodeIssue objects
        issues = []
        for issue_data in response_data.get('issues', []):
            try:
                issue = CodeIssue(
                    file_path=file_path,
                    line_number=issue_data.get('line_number', 0),
                    severity=Severity(issue_data.get('severity', 'info')),
                    category=IssueCategory(issue_data.get('category', 'logic')),
                    title=issue_data.get('title', ''),
                    description=issue_data.get('description', ''),
                    suggestion=issue_data.get('suggestion', '')
                )
                issues.append(issue)
            except Exception as e:
                logger.warning(f"Failed to parse issue: {e}")
        
        # Build the result
        result = AnalysisResult(
            file_path=file_path,
            overall_score=response_data.get('overall_score', 50.0),
            issues=issues,
            suggestions=response_data.get('suggestions', []),
            strengths=response_data.get('strengths', []),
            complexity_assessment=response_data.get('complexity_assessment', '')
        )
        
        logger.info(f"Analysis complete: Score {result.overall_score}/100, "
                   f"{len(result.issues)} issues found")
        
        return result
    
    def analyze_project(self, scan_results: Dict[str, Any], max_files: int = 10) -> List[AnalysisResult]:
        """
        Analyze multiple files from a project scan.
        
        Args:
            scan_results: Results from scanner
            max_files: Maximum number of files to analyze (for cost control)
            
        Returns:
            List of AnalysisResult objects
        """
        results = []
        analyzed_count = 0
        
        for file_meta in scan_results.get('files', []):
            if analyzed_count >= max_files:
                logger.info(f"Reached max file limit ({max_files})")
                break
            
            # Only analyze Python files for now
            if file_meta['language'] != 'Python':
                continue
            
            # Read the file content
            try:
                file_path = os.path.join(scan_results['root_path'], file_meta['path'])
                with open(file_path, 'r', encoding='utf-8') as f:
                    code = f.read()
                
                # Prepare context
                context = {
                    'imports': file_meta.get('imports', []),
                    'functions': file_meta.get('functions', []),
                    'classes': file_meta.get('classes', []),
                    'complexity': file_meta.get('complexity_score', 0)
                }
                
                # Analyze the code
                result = self.analyze_code(code, file_meta['path'], context)
                results.append(result)
                analyzed_count += 1
                
            except Exception as e:
                logger.error(f"Failed to analyze {file_meta['path']}: {e}")
        
        logger.info(f"Analyzed {analyzed_count} files")
        return results


def main():
    """Example usage of the AI engine"""
    # Example code to analyze
    example_code = '''
def calculate_total(items):
    total = 0
    for item in items:
        for price in prices:
            if price['id'] == item['id']:
                total += price['amount']
    return total

def get_user_data(user_id):
    query = f"SELECT * FROM users WHERE id={user_id}"
    return execute_query(query)
'''
    
    # Create AI engine
    engine = AIEngine()
    
    # Analyze the code
    result = engine.analyze_code(
        code=example_code,
        file_path="example.py",
        context={
            'imports': [],
            'functions': ['calculate_total', 'get_user_data'],
            'classes': [],
            'complexity': 15.5
        }
    )
    
    # Print results
    print(f"\n{'='*60}")
    print(f"ðŸ¤– AI Analysis Results")
    print(f"{'='*60}\n")
    print(f"Overall Score: {result.overall_score}/100\n")
    
    print(f"ðŸ”´ Issues Found: {len(result.issues)}")
    for issue in result.issues:
        print(f"\n  [{issue.severity.value.upper()}] {issue.title}")
        print(f"  Line {issue.line_number} | {issue.category.value}")
        print(f"  {issue.description}")
        print(f"  ðŸ’¡ Suggestion: {issue.suggestion}")
    
    print(f"\nâœ… Strengths:")
    for strength in result.strengths:
        print(f"  â€¢ {strength}")
    
    print(f"\nðŸ’¡ Suggestions:")
    for suggestion in result.suggestions:
        print(f"  â€¢ {suggestion}")


if __name__ == '__main__':
    main()
